{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f614058",
   "metadata": {},
   "source": [
    "## Валидация: выделение объектов из текста с помощью spacy\n",
    "\n",
    "прогоняем наш синтетический датасет через модель на основе SpaCy и считаем метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9552c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import spacy\n",
    "import re\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "from typing import List, Dict\n",
    "from spacy.matcher import Matcher, DependencyMatcher\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from openai import OpenAI\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lib_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "from library.final_metrics import evaluate_obj_attr_metrics, evaluate_ged_score\n",
    "\n",
    "\n",
    "# Путь к папке с датасетом\n",
    "DATASET_DIR = \"../dataset/dataset_validation_spacial\"\n",
    "VAL_SPLIT = 1.0 # на всем тестовом куске\n",
    "\n",
    "# можно ru_core_news_sm но она работает хуже\n",
    "nlp = spacy.load(\"ru_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbadcd8",
   "metadata": {},
   "source": [
    "## \"Модельная\" часть\n",
    "\n",
    "### Функции предобработки и разбора\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8b7d4",
   "metadata": {},
   "source": [
    "### Выделение объектов\n",
    "\n",
    "\n",
    "сразу добавлено что если после существительного идет номер, например \"ваза 1\" и далее есть \"ваза 2\" - то это два разных объекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86850f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_objects = {\n",
    "    \"справа\", \"слева\", \"рядом\", \"впереди\", \"сзади\", \"напротив\", \"внутри\",\n",
    "    \"снаружи\", \"близко\", \"далеко\", \"около\", \"возле\", \"между\", \"под\", \"над\"\n",
    "}\n",
    "\n",
    "\n",
    "def extract_objects(text):\n",
    "    doc = nlp(text)\n",
    "    objects = []\n",
    "    added = set()\n",
    "    skip_next = False\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        if token.pos_ == \"NOUN\" and token.lemma_.lower() not in pseudo_objects:\n",
    "            # Случай: существительное + число (\"ваза 1\")\n",
    "            if i + 1 < len(doc) and doc[i + 1].pos_ == \"NUM\":\n",
    "                combined = token.lemma_.lower() + ' ' + doc[i + 1].text\n",
    "                if combined not in added:\n",
    "                    objects.append(combined)\n",
    "                    added.add(combined)\n",
    "                skip_next = True\n",
    "            else:\n",
    "                # Без номера — сохраняем по лемме (нормализованной форме)\n",
    "                noun_lemma = token.lemma_.lower()\n",
    "                if noun_lemma not in added:\n",
    "                    objects.append(noun_lemma)\n",
    "                    added.add(noun_lemma)\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f92b47",
   "metadata": {},
   "source": [
    "### Выделение атрибутов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41ebeda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attributes(text, objects):\n",
    "    doc = nlp(text)\n",
    "    attr_map = defaultdict(set)\n",
    "\n",
    "    # Собираем нормализованные ключи объектов для сопоставления\n",
    "    normalized_objects = set()\n",
    "    for obj in objects:\n",
    "        parts = obj.split()\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            norm = nlp(parts[0])[0].lemma_ + ' ' + parts[1]\n",
    "            normalized_objects.add(norm)\n",
    "        else:\n",
    "            norm = nlp(obj)[0].lemma_\n",
    "            normalized_objects.add(norm)\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.pos_ != \"NOUN\":\n",
    "            continue\n",
    "\n",
    "        # Определяем имя объекта\n",
    "        object_key = None\n",
    "        next_token = doc[i + 1] if i + 1 < len(doc) else None\n",
    "        if next_token and next_token.pos_ == \"NUM\":\n",
    "            object_key = token.lemma_.lower() + ' ' + next_token.text\n",
    "        else:\n",
    "            object_key = token.lemma_.lower()\n",
    "\n",
    "        if object_key not in normalized_objects:\n",
    "            continue\n",
    "\n",
    "        # 1. Прямые прилагательные (amod) \n",
    "        # Для каждого объекта (token) ищем его потомков (children);\n",
    "        # Если потомок — прилагательное (ADJ) и имеет зависимость amod (атрибут);\n",
    "        # Смотрим, есть ли у прилагательного наречия (ADV) — например, \"очень\", \"слишком\";\n",
    "        # Склеиваем модификаторы + прилагательное и добавляем это как признак объекта.\n",
    "        for child in token.children:\n",
    "            if child.pos_ == \"ADJ\" and child.dep_ == \"amod\":\n",
    "                adv_mods = [adv.text.lower() for adv in child.children if adv.pos_ == \"ADV\"]\n",
    "                phrase = \" \".join(adv_mods + [child.text.lower()])\n",
    "                attr_map[object_key].add(phrase)\n",
    "\n",
    "        # 2. Причастия (acl) \n",
    "        # Этот блок обрабатывает причастия, которые выступают в роли определений к объекту \n",
    "        # то есть описывают действия или состояния, связанные с ним.\n",
    "        #\n",
    "        # Для каждого объекта (token) ищем его дочерние узлы (children);\n",
    "        # Если один из них — глагол (VERB) в форме причастия (VerbForm=Part) и его зависимость acl;\n",
    "        # Собираем модификаторы-причастия (очень, ещё, уже) и зависимые дополнения (трещинами, на полу, на вазу);\n",
    "        # Формируем фразу и добавляем её как признак.\n",
    "                \n",
    "        for child in token.children:\n",
    "\n",
    "            #if child.dep_ == \"acl\":\n",
    "            #    print(f\"\\n[DEBUG] Объект: {token.text} ({object_key})\")\n",
    "            #    print(f\"  Причастие: {child.text} ({child.dep_}, {child.pos_}, {child.morph})\")\n",
    "            #    for tok in child.children:\n",
    "            #        print(f\"    Дополнение: {tok.text} ({tok.dep_}, {tok.pos_}, {tok.morph}) -> предлог: {[c.text for c in tok.children if c.dep_ == 'case']}\")\n",
    "\n",
    "            \n",
    "            \n",
    "            if child.pos_ == \"VERB\" and \"Part\" in child.morph.get(\"VerbForm\") and child.dep_ == \"acl\":\n",
    "                adv_mods = [adv.text.lower() for adv in child.children if adv.pos_ == \"ADV\"]\n",
    "                \n",
    "                complements = []\n",
    "                for tok in child.children:\n",
    "                    if tok.dep_ in {\"obl\", \"obj\", \"nmod\", \"iobj\"} and tok.pos_ != \"NUM\":\n",
    "                        prep = [c.text.lower() for c in tok.children if c.dep_ == \"case\"]  # предлог\n",
    "                        if prep:\n",
    "                            phrase = \" \".join(prep + [tok.text.lower()])\n",
    "                        else:\n",
    "                            phrase = tok.text.lower()  # <- вот эта строка добавляет \"трещинами\"\n",
    "                        complements.append(phrase)\n",
    "\n",
    "                phrase = \" \".join(adv_mods + [child.text.lower()] + complements)\n",
    "                attr_map[object_key].add(phrase)                \n",
    "\n",
    "        # 3. Субъект при прилагательном/причастии \n",
    "        # Этот блок находит признаки, выраженные через предикативную конструкцию, \n",
    "        # где объект (существительное) является подлежащим (nsubj) прилагательного или причастия, \n",
    "        # стоящего в роли сказуемого.\n",
    "        # \n",
    "        # Ищем токен, который является подлежащим (dep_ == \"nsubj\")и зависит от прилагательного \n",
    "        # (ADJ) или причастия (VERB с VerbForm=Part)\n",
    "        # Считаем это head — прилагательное или причастие, описывающее объект\n",
    "        # Собираем наречия-модификаторы (например, очень, почти) и зависимые дополнения \n",
    "        # (например, на вазу, трещинами) и формируем полное описание признака и сохраняем.\n",
    "        if token.dep_ == \"nsubj\" and (token.head.pos_ == \"ADJ\" or (\"Part\" in token.head.morph.get(\"VerbForm\"))):\n",
    "            head = token.head\n",
    "            adv_mods = [adv.text.lower() for adv in head.children if adv.pos_ == \"ADV\"]\n",
    "            #complements = [tok.text.lower() for tok in head.children if tok.dep_ in {\"obl\", \"obj\", \"nmod\"}]\n",
    "            #complements = [tok.text.lower() for tok in head.children if tok.dep_ in {\"obl\", \"obj\", \"nmod\"} and tok.pos_ != \"NUM\"]\n",
    "            \n",
    "            complements = []\n",
    "            for tok in head.children:\n",
    "                if tok.dep_ in {\"obl\", \"obj\", \"nmod\"} and tok.pos_ != \"NUM\":\n",
    "                    prep = [c.text.lower() for c in tok.children if c.dep_ == \"case\"]  # предлог\n",
    "                    phrase = \" \".join(prep + [tok.text.lower()])\n",
    "                    complements.append(phrase)\n",
    "\n",
    "            \n",
    "            phrase = \" \".join(adv_mods + [head.text.lower()] + complements)\n",
    "            attr_map[object_key].add(phrase)\n",
    "\n",
    "        # 4. Признак после союза (например: \"пластмассовый стул 2 но белый\")\n",
    "        # Этот блок реализует эвристику для распознавания признаков, стоящих после объекта, но не связанных напрямую синтаксически из-за союзов, \n",
    "        # которые \"разрывают\" связь (например, но, а, зато).\n",
    "        # \n",
    "        # Проверяет, что текущий токен — это существительное (NOUN), за которым идёт число (NUM) - \n",
    "        # мы имеем объект вида стул 2. Затем на расстоянии +2 токена проверяется, есть ли союз но, а, зато\n",
    "        # И за союзом — прилагательное (ADJ), которое трактуется как дополнительный признак к объекту\n",
    "        if (i + 3 < len(doc) and token.pos_ == \"NOUN\" and next_token and next_token.pos_ == \"NUM\"):\n",
    "            possible_adj = doc[i + 3]\n",
    "            conjunction = doc[i + 2]\n",
    "            if (conjunction.text.lower() in {\"но\", \"а\", \"зато\"} and possible_adj.pos_ == \"ADJ\" ):\n",
    "                object_key = token.lemma_.lower() + ' ' + next_token.text\n",
    "                attr_map[object_key].add(possible_adj.text.lower())     \n",
    "                \n",
    "                \n",
    "        # --- 5. Цепочка прилагательных перед существительным (включая ADV-модификаторы) ---\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            j = i - 1\n",
    "            collected = []\n",
    "            while j >= 0:\n",
    "                t = doc[j]\n",
    "                if t.pos_ == \"ADJ\":\n",
    "                    # ищем наречия, модифицирующие прилагательное\n",
    "                    adv_mods = [adv.text.lower() for adv in t.children if adv.pos_ == \"ADV\"]\n",
    "                    phrase = \" \".join(adv_mods + [t.text.lower()])\n",
    "                    collected.insert(0, phrase)\n",
    "                elif t.pos_ == \"ADV\":\n",
    "                    pass  # ADV может быть частью модификатора — обработается выше\n",
    "                elif t.text.lower() in {\"и\", \",\"} or t.pos_ == \"CCONJ\" or t.pos_ == \"PUNCT\":\n",
    "                    pass\n",
    "                else:\n",
    "                    break\n",
    "                j -= 1\n",
    "            if collected:\n",
    "                attr_map[object_key].update(collected)        \n",
    "                \n",
    "        # 6. Признаки при существительном, выступающем подлежащим (nsubj), особенно если глагол не причастие\n",
    "        if token.dep_ == \"nsubj\" and token.pos_ == \"NOUN\":\n",
    "            # проверим, есть ли у подлежащего прилагательные (amod)\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"amod\" and child.pos_ == \"ADJ\":\n",
    "                    adv_mods = [adv.text.lower() for adv in child.children if adv.pos_ == \"ADV\"]\n",
    "                    phrase = \" \".join(adv_mods + [child.text.lower()])\n",
    "                    attr_map[object_key].add(phrase)                \n",
    "\n",
    "        # 7. Признаки у вложенных объектов (например, \"стол с пожелтевшими фотографиями\") \n",
    "        for token in doc:\n",
    "            if token.lemma_.lower() in normalized_objects:\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"amod\" and child.pos_ in {\"ADJ\", \"VERB\"}:\n",
    "                        adv_mods = [adv.text.lower() for adv in child.children if adv.pos_ == \"ADV\"]\n",
    "                        phrase = \" \".join(adv_mods + [child.text.lower()])\n",
    "                        object_key = token.lemma_.lower()\n",
    "                        attr_map[object_key].add(phrase)                    \n",
    "                    \n",
    "    return {k: sorted(v) for k, v in attr_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b3bb6",
   "metadata": {},
   "source": [
    "### Выделение пространственных связей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f80d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_objects(objects):\n",
    "    norms = {}\n",
    "    for obj in objects:\n",
    "        parts = obj.split()\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            norms[obj] = f\"{nlp(parts[0])[0].lemma_} {parts[1]}\"\n",
    "        else:\n",
    "            norms[obj] = nlp(obj)[0].lemma_\n",
    "    return norms\n",
    "\n",
    "def find_matching_object(token, object_norms):\n",
    "    lemma = token.lemma_.lower()\n",
    "    for orig, norm in object_norms.items():\n",
    "        if lemma == norm:\n",
    "            return orig\n",
    "        if token.i + 1 < len(token.doc):\n",
    "            next_tok = token.doc[token.i + 1]\n",
    "            combined = f\"{lemma} {next_tok.text}\"\n",
    "            if combined == norm:\n",
    "                return orig\n",
    "        if token.i - 1 >= 0:\n",
    "            prev_tok = token.doc[token.i - 1]\n",
    "            combined = f\"{prev_tok.lemma_} {token.text}\"\n",
    "            if combined == norm:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "def collect_conj_appos_group(token):\n",
    "    group = set()\n",
    "\n",
    "    def dfs(tok):\n",
    "        if tok in group:\n",
    "            return\n",
    "        group.add(tok)\n",
    "        for child in tok.children:\n",
    "            if child.dep_ in {\"conj\", \"appos\"}:\n",
    "                dfs(child)\n",
    "        if tok.dep_ in {\"conj\", \"appos\"}:\n",
    "            dfs(tok.head)\n",
    "\n",
    "    dfs(token)\n",
    "    return sorted(group, key=lambda x: x.i)\n",
    "\n",
    "def build_full_prep_phrase(prep_token, obj_token):\n",
    "    parts = [prep_token.text.lower()]\n",
    "    for child in obj_token.children:\n",
    "        if child.dep_ == \"case\":\n",
    "            parts.append(child.text.lower())\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def extract_spatial_relations(text, objects):\n",
    "    doc = nlp(text)\n",
    "    matcher = DependencyMatcher(nlp.vocab)\n",
    "    norms = normalize_objects(objects)\n",
    "    relations = set()\n",
    "\n",
    "    pattern_verb = [\n",
    "        {\"RIGHT_ID\": \"verb\", \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}},\n",
    "        {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"nsubj\", \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}},\n",
    "        {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"obl\", \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"obl\", \"nmod\"]}}},\n",
    "        {\"LEFT_ID\": \"obl\", \"REL_OP\": \">\", \"RIGHT_ID\": \"prep\", \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"case\", \"flat\", \"fixed\"]}}}\n",
    "    ]\n",
    "\n",
    "    pattern_nearby = [\n",
    "        {\"RIGHT_ID\": \"verb\", \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}},\n",
    "        {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"prep_adv\", \"RIGHT_ATTRS\": {\"DEP\": \"advmod\", \"POS\": \"ADV\"}},\n",
    "        {\"LEFT_ID\": \"prep_adv\", \"REL_OP\": \">\", \"RIGHT_ID\": \"obj\", \"RIGHT_ATTRS\": {\"DEP\": \"obl\"}},\n",
    "        {\"LEFT_ID\": \"obj\", \"REL_OP\": \">\", \"RIGHT_ID\": \"prep\", \"RIGHT_ATTRS\": {\"DEP\": \"case\"}},\n",
    "        {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"subj\", \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}},\n",
    "    ]\n",
    "\n",
    "    matcher.add(\"SPATIAL_VERB\", [pattern_verb])\n",
    "    matcher.add(\"SPATIAL_NEARBY\", [pattern_nearby])\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    def build_full_prep(tok):\n",
    "        parts = [tok.text.lower()]\n",
    "        parts += sorted([child.text.lower() for child in tok.children if child.dep_ in {\"fixed\", \"flat\", \"case\"}], key=lambda x: x)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    for match_id, tokens in matches:\n",
    "        match_label = nlp.vocab.strings[match_id]\n",
    "\n",
    "        if match_label == \"SPATIAL_VERB\":\n",
    "            verb, nsubj, obl, prep = [doc[i] for i in tokens]\n",
    "            prep_phrase = build_full_prep(prep)\n",
    "\n",
    "            subj_group = collect_conj_appos_group(nsubj)\n",
    "            obl_group = collect_conj_appos_group(obl)\n",
    "\n",
    "            subj_objs = [find_matching_object(tok, norms) for tok in subj_group]\n",
    "            obl_objs = [find_matching_object(tok, norms) for tok in obl_group]\n",
    "\n",
    "            subj_objs = [obj for obj in subj_objs if obj]\n",
    "            obl_objs = [obj for obj in obl_objs if obj]\n",
    "\n",
    "            for subj_obj in subj_objs:\n",
    "                for obl_obj in obl_objs:\n",
    "                    if subj_obj != obl_obj:\n",
    "                        relations.add((subj_obj, prep_phrase, obl_obj))\n",
    "\n",
    "        elif match_label == \"SPATIAL_NEARBY\":\n",
    "            verb, prep_adv, obj, prep, subj = [doc[i] for i in tokens]\n",
    "            #prep_phrase = build_full_prep(prep_adv)\n",
    "            prep_phrase = build_full_prep_phrase(prep_adv, obj)\n",
    "\n",
    "            subj_group = collect_conj_appos_group(subj)\n",
    "            subj_objs = [find_matching_object(tok, norms) for tok in subj_group]\n",
    "            subj_objs = [obj for obj in subj_objs if obj]\n",
    "\n",
    "            obl_obj = find_matching_object(obj, norms)\n",
    "\n",
    "            if obl_obj:\n",
    "                for subj_obj in subj_objs:\n",
    "                    if subj_obj != obl_obj:\n",
    "                        relations.add((subj_obj, prep_phrase, obl_obj))\n",
    "\n",
    "    return list(relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a178767",
   "metadata": {},
   "source": [
    "## Итоговая сборка\n",
    "\n",
    "текст -> сценический граф \n",
    "\n",
    "```\n",
    "{\n",
    "    \"scene\": {\n",
    "        \"location\": \"автосервис\",\n",
    "        \"objects\": [\n",
    "            {\"гаечный ключ\": [\"металлический\"]},\n",
    "            {\"домкрат\": [\"металлический\", \"тяжелый\", \"прочный\"]},\n",
    "            {\"аккумулятор\": []}\n",
    "        ],\n",
    "        \"relations\": [\n",
    "            [\"аккумулятор\", \"рядом с\", \"гаечный ключ\"],\n",
    "            [\"гаечный ключ\", \"на\", \"домкрат\"]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "location мы не предсказыаем, поэтому оно \"неизвестно\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9354c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_model(description, location=\"неизвестно\"):\n",
    "    # Извлечение предсказаний\n",
    "    objects = extract_objects(description)\n",
    "    attributes = extract_attributes(description, objects)\n",
    "    relations = extract_spatial_relations(description, objects)      \n",
    "    \n",
    "    # Приведение предсказанных атрибутов к полному списку объектов\n",
    "    completed_attrs = [{obj: attributes.get(obj, [])} for obj in objects]\n",
    "\n",
    "    scene = dict()\n",
    "    scene[\"location\"] = location\n",
    "    scene[\"objects\"] = completed_attrs\n",
    "    scene[\"relations\"] = relations\n",
    "    \n",
    "    return {\"scene\": scene}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b43ebf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224eae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "673fa8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем все jsonl-файлы из датасета\n",
    "def load_dataset(path: str) -> List[Dict]:\n",
    "    dataset = []\n",
    "    for filename in glob.glob(os.path.join(path, \"*.jsonl\")):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                dataset.append(json.loads(line))\n",
    "    return dataset\n",
    "\n",
    "# Получаем .05 данных\n",
    "def sample_validation_split(dataset: List[Dict], fraction: float = 0.05) -> List[Dict]:\n",
    "    sample_size = max(1, int(len(dataset) * fraction))\n",
    "    return random.sample(dataset, sample_size)\n",
    "\n",
    "# Обработка + сбор метрик\n",
    "def evaluate_on_validation_set(dataset: List[Dict]) -> Dict[str, float]:\n",
    "    metrics_accumulator = defaultdict(list)\n",
    "    \n",
    "    for item in tqdm(dataset, ncols=80):\n",
    "        src_text = item[\"description\"]        \n",
    "        if not src_text:\n",
    "            print(\"Пустое или отсутствующее поле 'description' в элементе:\")\n",
    "            print(item)\n",
    "            continue\n",
    "        \n",
    "        pred = spacy_model(src_text)\n",
    "        label = {\"scene\": item[\"scene\"]}\n",
    "        \n",
    "        # Оценка для базовых метрик\n",
    "        metrics = evaluate_obj_attr_metrics(pred, label)\n",
    "        for k, v in metrics.items():\n",
    "            metrics_accumulator[k].append(v)\n",
    "\n",
    "        # Оценка с точки зрения графа целиком\n",
    "        metrics = evaluate_ged_score(pred, label)\n",
    "        for k, v in metrics.items():\n",
    "            metrics_accumulator[k].append(v)\n",
    "            \n",
    "            \n",
    "    # Усреднение по всем примерам\n",
    "    return {k: round(sum(vs) / len(vs), 4) for k, vs in metrics_accumulator.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0977eed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 250/250 [07:48<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results on 250 samples:\n",
      "f1_objects: 0.9505\n",
      "f1_attributes_macro: 0.6571\n",
      "f1_attributes_weighted: 0.8502\n",
      "f1_global_obj_attr_pairs: 1.0\n",
      "f1_combined_simple: 0.8038\n",
      "f1_combined_weighted: 0.9012\n",
      "GED_score: 0.5398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_data = load_dataset(DATASET_DIR)\n",
    "val_data = sample_validation_split(all_data, VAL_SPLIT)\n",
    "final_metrics = evaluate_on_validation_set(val_data)\n",
    "\n",
    "print(\"Validation results on\", len(all_data), \"samples:\")\n",
    "for k, v in final_metrics.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af80fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac493ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724dd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f5151b99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd390d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
