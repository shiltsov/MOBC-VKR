{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5382978f",
   "metadata": {},
   "source": [
    "## Генерация датасата (Obj+Attr_Spacial) - II\n",
    "\n",
    "Второй этап генерации датасета. По имеющимся бачам (строки текста) делаем сначала нормализацию текста с помощью LLM и далее  разбор с помощью SpaCy, генерируем json нужного вида и пишем в jsonl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb3989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, DependencyMatcher\n",
    "\n",
    "import logging\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "#print(API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74f22900",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "Ты — система нормализации описаний сцены.\n",
    "\n",
    "Твоя задача — преобразовать текст сцены так, чтобы:\n",
    "\n",
    "1. Все объекты (например, \"ваза\", \"стул\", \"ящик\", \"цветы\", \"стол\", \"комната\") были явно названы.\n",
    "2. Если **один и тот же тип объекта упоминается более одного раза** (например, несколько ваз или стульев), \n",
    "   ты должен **нумеровать их**: \"ваза 1\", \"ваза 2\", \"стул 1\" и т.п.  \n",
    "   ❗️**Если объект встречается в тексте только один раз (или упоминается под разными формами, но это один \n",
    "    и тот же объект)** — **нумерацию добавлять НЕ НУЖНО**.\n",
    "3. Если объект упоминается частично или через эллипсис (например: \"стояли белая ваза и синяя пустая\"), \n",
    "   ты должен восстановить полное название объекта: \"стояли белая ваза 1 и синяя пустая ваза 2\"\n",
    "4. Если объект упоминается через местоимение или косвенно (например: \"она\", \"эта ваза\", \"вазой\", \n",
    "   \"с ней\", \"возле неё\", \"на нём\" и т.п.), ты должен заменить это на ссылку на соответствующий объект или локацию — \n",
    "   например, \"с ней\" → \"с вазой 1\", \"на нём\" → \"на столике\", \"она\" → \"ваза 1\" и т.п.   \n",
    "4б. Если в тексте описывается пространственное отношение между двумяи более объектами (например, \"между ними\", \n",
    "   \"между деревьями\"), и эти объекты были ранее явно указаны и пронумерованы (например, \"дерево 1\" \n",
    "    и \"дерево 2\"), ты должен явно указать, между какими объектами происходит это взаимодействие:  \n",
    "    например, \"между деревьями\" → \"между деревом 1 и деревом 2\".\n",
    "5. Если в тексте используются указания на место через местоимения или конструкции типа \"на нём\", \"в ней\", \n",
    "   \"под ним\", \"возле неё\" и т.п., ты должен заменить их на явное упоминание объекта, к которому они относятся.  \n",
    "   Например, \"на нём стояла ваза\" → \"на столике стояла ваза\", если ранее был упомянут \"столик\".\n",
    "6. Все предикативные описания (например, \"выглядит дорого\", \"казалась грязной\", \"был тяжёлым\") ты должен \n",
    "   заменить на атрибутивные прилагательные (например, \"дорогая\", \"грязная\", \"тяжёлый\") и добавить их в \n",
    "   описание соответствующего объекта.\n",
    "7. Если признаки объекта выражены в форме наречий (например: \"выглядит стильно\", \"дорого\"), ты должен \n",
    "   преобразовать их в соответствующие прилагательные (\"стильная\", \"дорогая\").\n",
    "8. Если признаки находятся в отдельных оборотах (например: \"ящик, стоящий у стены\", \"ваза, обитая бархатом\"), \n",
    "   ты должен преобразовать их в прилагательные и включить в описание объекта (\"стоящий у стены ящик\", \n",
    "   \"бархатная ваза\").\n",
    "9. Если признаки объектов выражены в других предложениях (например: \"Он был тяжёлым\"), ты должен сопоставить \n",
    "   это с соответствующим объектом и сделать описание полным.\n",
    "10. Сохраняй общий стиль, структуру и естественность текста, но обеспечивай, чтобы каждое упоминание \n",
    "   объектов было максимально полным, со всеми признаками, явно включёнными в их описание.\n",
    "11. Если в тексте есть групповые перечисления объектов (например: \"три вазы: синяя, белая и жёлтая\"), \n",
    "    ты должен убрать обобщающее существительное (\"три вазы\") и превратить его в перечисление конкретных \n",
    "    объектов:  \"синяя ваза 1, белая ваза 2 и жёлтая ваза 3\".\n",
    "    Также, если перед перечислением есть другие объекты (например: \"стол и два стула: один сломан, \n",
    "    другой новый\"), ты должен преобразовать всю конструкцию в единый список объектов:  \n",
    "    \"старый стол, сломанный стул 1 и новый стул 2\".\n",
    "\n",
    "---\n",
    "\n",
    "### Примеры (корректная нормализация):\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"На столе стояла белая ваза с цветами и синяя пустая. Стул находился перед вазой с цветами. Обе вазы выглядели очень дорого и стильно.\"\n",
    "\n",
    "**Результат:**  \n",
    "\"На столе стояла белая стильная дорогая ваза 1 с цветами и синяя пустая стильная дорогая ваза 2. Стул находился перед вазой 1 с цветами.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"На подоконнике стоял чайник. Он был старым, но казался прочным.\"\n",
    "\n",
    "**Результат:**  \n",
    "\"На подоконнике стоял старый прочный чайник.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"Рядом с диваном стоял ящик, покрытый тканью. Он выглядел уютно.\"\n",
    "\n",
    "**Результат:**  \n",
    "\"Рядом с диваном стоял уютный тканевый ящик.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"На стене висели две картины. одна маленькая а другая большая\"\n",
    "\n",
    "**Результат:**  \n",
    "\"На стене висела большая картина 1 и маленькая картина 2.\"\n",
    "\n",
    "---\n",
    "\n",
    "Исходный текст:\n",
    "\"Рядом с креслом находился пыльный столик. На нём — ваза, очень тяжёлая, и другая — более лёгкая, \n",
    "но с разбитым краем. У тяжёлой вазы была едва заметная трещина.\"\n",
    "\n",
    "Результат:\n",
    "\"Рядом с креслом находился пыльный столик. На столике стояла тяжёлая ваза 1 с едва заметной \n",
    "трещиной и более лёгкая ваза 2 с разбитым краем.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"У окна стояли два стула — один зелёный, другой с мягкой обивкой. Они казались новыми.\"\n",
    "\n",
    "**Результат:**  \n",
    "\"У окна стояли новый зеленый стул 1 и новый стул 2 с мягкой обивкой.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"В спальне находились кровать, тумбочка и шкаф. Шкаф был высокий, покрытый резьбой и слегка покосившийся. Кровать была заправлена, а тумбочка стояла пустая.\"\n",
    "\n",
    "**Результат:**  \n",
    "\"В спальне находились заправленная кровать, пустая тумбочка и высокий резной слегка покосившийся шкаф.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**\n",
    "\"На столе стояла высокая стеклянная ваза и рядом с ней — другая, покрытая трещинами. Стеклянная ваза была очень изящной и сияющей. Возле нее сидела серая умная кошка.\"\n",
    "\n",
    "**Результат:**\n",
    "\"На столе стояла высокая изящная сияющая стеклянная ваза 1, и рядом с вазой 1 — покрытая трещинами ваза 2. Возле вазы 1 сидела серая умная кошка.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**\n",
    "\"На сцене стоит старый деревянный стол и два стула: один сломан, другой — новый.\"\n",
    "\n",
    "**Результат:**\n",
    "\"На сцене стоит старый деревянный стол, сломанный стул 1 и новый стул 2.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Антипримеры (ошибочная нормализация):\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"Под большим круглым столом прятался кот с поцарапанным ухом. На столе стояла ваза с золотыми узорами, слегка наклонённая вбок.\"\n",
    "\n",
    "** Неправильно:**  \n",
    "\"Под большим круглым столом прятался кот 1 с поцарапанным ухом. На столе стояла слегка наклонённая вбок ваза 1 с золотыми узорами.\"\n",
    "\n",
    "** Правильно:**  \n",
    "\"Под большим круглым столом прятался кот с поцарапанным ухом. На столе стояла слегка наклонённая вбок ваза с золотыми узорами.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"Возле окна стояла старая этажерка с книгами. На ней лежал фонарь.\"\n",
    "\n",
    "** Неправильно:**  \n",
    "\"Возле окна стояла старая этажерка 1 с книгами. На ней лежал фонарь 1.\"\n",
    "\n",
    "** Правильно:**  \n",
    "\"Возле окна стояла старая этажерка с книгами. На ней лежал фонарь.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"На полке стояла мягкая игрушка в виде медведя. Она выглядела новой и чистой.\"\n",
    "\n",
    "** Неправильно:**  \n",
    "\"На полке стояла новая чистая мягкая игрушка 1 в виде медведя.\"\n",
    "\n",
    "** Правильно:**  \n",
    "\"На полке стояла новая чистая мягкая игрушка в виде медведя.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Исходный текст:**  \n",
    "\"На диване спала собака. Она была пушистой и ласковой.\"\n",
    "\n",
    "** Неправильно:**  \n",
    "\"На диване спала пушистая ласковая собака 1.\"\n",
    "\n",
    "** Правильно:**  \n",
    "\"На диване спала пушистая ласковая собака.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d9e9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scene_text(text: str, PROMPT: str, API_KEY: str) -> str:\n",
    "    \"\"\"\n",
    "    Нормализует текст сцены: заменяет эллипсис, разрешает coreference, нумерует объекты.\n",
    "    \n",
    "    :param text: исходный текст описания сцены\n",
    "    :param PROMPT: текстовая инструкция с примерами (см. ниже)\n",
    "    :param API_KEY: OpenAI API Key\n",
    "    :return: нормализованный текст\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Ты помощник по нормализации текстов сцен.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{PROMPT.strip()}\\n\\nТекст:\\n{text.strip()}\"}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=1000            \n",
    "        )        \n",
    "\n",
    "        normalized_text = response.choices[0].message.content.strip()\n",
    "        return normalized_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обращении к OpenAI API: {e}\")\n",
    "        return text  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "909f72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_objects = {\n",
    "    \"справа\", \"слева\", \"рядом\", \"впереди\", \"сзади\", \"напротив\", \"внутри\",\n",
    "    \"снаружи\", \"близко\", \"далеко\", \"около\", \"возле\", \"между\", \"под\", \"над\"\n",
    "}\n",
    "\n",
    "\n",
    "def extract_objects(text):\n",
    "    doc = nlp(text)\n",
    "    objects = []\n",
    "    added = set()\n",
    "    skip_next = False\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        if token.pos_ == \"NOUN\" and token.lemma_.lower() not in pseudo_objects:\n",
    "            # Случай: существительное + число (\"ваза 1\")\n",
    "            if i + 1 < len(doc) and doc[i + 1].pos_ == \"NUM\":\n",
    "                combined = token.lemma_.lower() + ' ' + doc[i + 1].text\n",
    "                if combined not in added:\n",
    "                    objects.append(combined)\n",
    "                    added.add(combined)\n",
    "                skip_next = True\n",
    "            else:\n",
    "                # Без номера — сохраняем по лемме (нормализованной форме)\n",
    "                noun_lemma = token.lemma_.lower()\n",
    "                if noun_lemma not in added:\n",
    "                    objects.append(noun_lemma)\n",
    "                    added.add(noun_lemma)\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "132379a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attributes(text, objects):\n",
    "    doc = nlp(text)\n",
    "    attr_map = defaultdict(set)\n",
    "\n",
    "    # Собираем нормализованные ключи объектов для сопоставления\n",
    "    normalized_objects = set()\n",
    "    for obj in objects:\n",
    "        parts = obj.split()\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            norm = nlp(parts[0])[0].lemma_ + ' ' + parts[1]\n",
    "            normalized_objects.add(norm)\n",
    "        else:\n",
    "            norm = nlp(obj)[0].lemma_\n",
    "            normalized_objects.add(norm)\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.pos_ != \"NOUN\":\n",
    "            continue\n",
    "\n",
    "        # Определяем имя объекта\n",
    "        object_key = None\n",
    "        next_token = doc[i + 1] if i + 1 < len(doc) else None\n",
    "        if next_token and next_token.pos_ == \"NUM\":\n",
    "            object_key = token.lemma_.lower() + ' ' + next_token.text\n",
    "        else:\n",
    "            object_key = token.lemma_.lower()\n",
    "\n",
    "        if object_key not in normalized_objects:\n",
    "            continue\n",
    "\n",
    "        # 1. Прямые прилагательные (amod) \n",
    "        # Для каждого объекта (token) ищем его потомков (children);\n",
    "        # Если потомок — прилагательное (ADJ) и имеет зависимость amod (атрибут);\n",
    "        # Смотрим, есть ли у прилагательного наречия (ADV) — например, \"очень\", \"слишком\";\n",
    "        # Склеиваем модификаторы + прилагательное и добавляем это как признак объекта.\n",
    "        for child in token.children:\n",
    "            if child.pos_ == \"ADJ\" and child.dep_ == \"amod\":\n",
    "                adv_mods = [adv.text.lower() for adv in child.children if adv.pos_ == \"ADV\"]\n",
    "                phrase = \" \".join(adv_mods + [child.text.lower()])\n",
    "                attr_map[object_key].add(phrase)\n",
    "\n",
    "        # 2. Причастия (acl) \n",
    "        # Этот блок обрабатывает причастия, которые выступают в роли определений к объекту \n",
    "        # то есть описывают действия или состояния, связанные с ним.\n",
    "        #\n",
    "        # Для каждого объекта (token) ищем его дочерние узлы (children);\n",
    "        # Если один из них — глагол (VERB) в форме причастия (VerbForm=Part) и его зависимость acl;\n",
    "        # Собираем модификаторы-причастия (очень, ещё, уже) и зависимые дополнения (трещинами, на полу, на вазу);\n",
    "        # Формируем фразу и добавляем её как признак.\n",
    "                \n",
    "        for child in token.children:\n",
    "\n",
    "            #if child.dep_ == \"acl\":\n",
    "            #    print(f\"\\n[DEBUG] Объект: {token.text} ({object_key})\")\n",
    "            #    print(f\"  Причастие: {child.text} ({child.dep_}, {child.pos_}, {child.morph})\")\n",
    "            #    for tok in child.children:\n",
    "            #        print(f\"    Дополнение: {tok.text} ({tok.dep_}, {tok.pos_}, {tok.morph}) -> предлог: {[c.text for c in tok.children if c.dep_ == 'case']}\")\n",
    "\n",
    "            \n",
    "            \n",
    "            if child.pos_ == \"VERB\" and \"Part\" in child.morph.get(\"VerbForm\") and child.dep_ == \"acl\":\n",
    "                adv_mods = [adv.text.lower() for adv in child.children if adv.pos_ == \"ADV\"]\n",
    "                \n",
    "                complements = []\n",
    "                for tok in child.children:\n",
    "                    if tok.dep_ in {\"obl\", \"obj\", \"nmod\", \"iobj\"} and tok.pos_ != \"NUM\":\n",
    "                        prep = [c.text.lower() for c in tok.children if c.dep_ == \"case\"]  # предлог\n",
    "                        if prep:\n",
    "                            phrase = \" \".join(prep + [tok.text.lower()])\n",
    "                        else:\n",
    "                            phrase = tok.text.lower()  # <- вот эта строка добавляет \"трещинами\"\n",
    "                        complements.append(phrase)\n",
    "\n",
    "                phrase = \" \".join(adv_mods + [child.text.lower()] + complements)\n",
    "                attr_map[object_key].add(phrase)                \n",
    "\n",
    "        # 3. Субъект при прилагательном/причастии \n",
    "        # Этот блок находит признаки, выраженные через предикативную конструкцию, \n",
    "        # где объект (существительное) является подлежащим (nsubj) прилагательного или причастия, \n",
    "        # стоящего в роли сказуемого.\n",
    "        # \n",
    "        # Ищем токен, который является подлежащим (dep_ == \"nsubj\")и зависит от прилагательного \n",
    "        # (ADJ) или причастия (VERB с VerbForm=Part)\n",
    "        # Считаем это head — прилагательное или причастие, описывающее объект\n",
    "        # Собираем наречия-модификаторы (например, очень, почти) и зависимые дополнения \n",
    "        # (например, на вазу, трещинами) и формируем полное описание признака и сохраняем.\n",
    "        if token.dep_ == \"nsubj\" and (token.head.pos_ == \"ADJ\" or (\"Part\" in token.head.morph.get(\"VerbForm\"))):\n",
    "            head = token.head\n",
    "            adv_mods = [adv.text.lower() for adv in head.children if adv.pos_ == \"ADV\"]\n",
    "            #complements = [tok.text.lower() for tok in head.children if tok.dep_ in {\"obl\", \"obj\", \"nmod\"}]\n",
    "            #complements = [tok.text.lower() for tok in head.children if tok.dep_ in {\"obl\", \"obj\", \"nmod\"} and tok.pos_ != \"NUM\"]\n",
    "            \n",
    "            complements = []\n",
    "            for tok in child.children:\n",
    "                if tok.dep_ in {\"obl\", \"obj\", \"nmod\"} and tok.pos_ != \"NUM\":\n",
    "                    prep = [c.text.lower() for c in tok.children if c.dep_ == \"case\"]  # предлог\n",
    "                    phrase = \" \".join(prep + [tok.text.lower()])\n",
    "                    complements.append(phrase)\n",
    "            \n",
    "            \n",
    "            phrase = \" \".join(adv_mods + [head.text.lower()] + complements)\n",
    "            attr_map[object_key].add(phrase)\n",
    "\n",
    "        # 4. Признак после союза (например: \"пластмассовый стул 2 но белый\")\n",
    "        # Этот блок реализует эвристику для распознавания признаков, стоящих после объекта, но не связанных напрямую синтаксически из-за союзов, \n",
    "        # которые \"разрывают\" связь (например, но, а, зато).\n",
    "        # \n",
    "        # Проверяет, что текущий токен — это существительное (NOUN), за которым идёт число (NUM) - \n",
    "        # мы имеем объект вида стул 2. Затем на расстоянии +2 токена проверяется, есть ли союз но, а, зато\n",
    "        # И за союзом — прилагательное (ADJ), которое трактуется как дополнительный признак к объекту\n",
    "        if (i + 3 < len(doc) and token.pos_ == \"NOUN\" and next_token and next_token.pos_ == \"NUM\"):\n",
    "            possible_adj = doc[i + 3]\n",
    "            conjunction = doc[i + 2]\n",
    "            if (conjunction.text.lower() in {\"но\", \"а\", \"зато\"} and possible_adj.pos_ == \"ADJ\" ):\n",
    "                object_key = token.lemma_.lower() + ' ' + next_token.text\n",
    "                attr_map[object_key].add(possible_adj.text.lower())     \n",
    "                \n",
    "                \n",
    "        # --- 5. Цепочка прилагательных перед существительным (включая ADV-модификаторы) ---\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            j = i - 1\n",
    "            collected = []\n",
    "            while j >= 0:\n",
    "                t = doc[j]\n",
    "                if t.pos_ == \"ADJ\":\n",
    "                    # ищем наречия, модифицирующие прилагательное\n",
    "                    adv_mods = [adv.text.lower() for adv in t.children if adv.pos_ == \"ADV\"]\n",
    "                    phrase = \" \".join(adv_mods + [t.text.lower()])\n",
    "                    collected.insert(0, phrase)\n",
    "                elif t.pos_ == \"ADV\":\n",
    "                    pass  # ADV может быть частью модификатора — обработается выше\n",
    "                elif t.text.lower() in {\"и\", \",\"} or t.pos_ == \"CCONJ\" or t.pos_ == \"PUNCT\":\n",
    "                    pass\n",
    "                else:\n",
    "                    break\n",
    "                j -= 1\n",
    "            if collected:\n",
    "                attr_map[object_key].update(collected)    \n",
    "                \n",
    "        # 6. Признаки при существительном, выступающем подлежащим (nsubj), особенно если глагол не причастие\n",
    "        if token.dep_ == \"nsubj\" and token.pos_ == \"NOUN\":\n",
    "            # проверим, есть ли у подлежащего прилагательные (amod)\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"amod\" and child.pos_ == \"ADJ\":\n",
    "                    adv_mods = [adv.text.lower() for adv in child.children if adv.pos_ == \"ADV\"]\n",
    "                    phrase = \" \".join(adv_mods + [child.text.lower()])\n",
    "                    attr_map[object_key].add(phrase)                \n",
    "\n",
    "        # 7. Признаки у вложенных объектов (например, \"стол с пожелтевшими фотографиями\") \n",
    "        for token in doc:\n",
    "            if token.lemma_.lower() in normalized_objects:\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"amod\" and child.pos_ in {\"ADJ\", \"VERB\"}:\n",
    "                        adv_mods = [adv.text.lower() for adv in child.children if adv.pos_ == \"ADV\"]\n",
    "                        phrase = \" \".join(adv_mods + [child.text.lower()])\n",
    "                        object_key = token.lemma_.lower()\n",
    "                        attr_map[object_key].add(phrase)                  \n",
    "                \n",
    "    return {k: sorted(v) for k, v in attr_map.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8ed18468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_objects(objects):\n",
    "    norms = {}\n",
    "    for obj in objects:\n",
    "        parts = obj.split()\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            norms[obj] = f\"{nlp(parts[0])[0].lemma_} {parts[1]}\"\n",
    "        else:\n",
    "            norms[obj] = nlp(obj)[0].lemma_\n",
    "    return norms\n",
    "\n",
    "def find_matching_object(token, object_norms):\n",
    "    lemma = token.lemma_.lower()\n",
    "    for orig, norm in object_norms.items():\n",
    "        if lemma == norm:\n",
    "            return orig\n",
    "        if token.i + 1 < len(token.doc):\n",
    "            next_tok = token.doc[token.i + 1]\n",
    "            combined = f\"{lemma} {next_tok.text}\"\n",
    "            if combined == norm:\n",
    "                return orig\n",
    "        if token.i - 1 >= 0:\n",
    "            prev_tok = token.doc[token.i - 1]\n",
    "            combined = f\"{prev_tok.lemma_} {token.text}\"\n",
    "            if combined == norm:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "def collect_conj_appos_group(token):\n",
    "    group = set()\n",
    "\n",
    "    def dfs(tok):\n",
    "        if tok in group:\n",
    "            return\n",
    "        group.add(tok)\n",
    "        for child in tok.children:\n",
    "            if child.dep_ in {\"conj\", \"appos\"}:\n",
    "                dfs(child)\n",
    "        if tok.dep_ in {\"conj\", \"appos\"}:\n",
    "            dfs(tok.head)\n",
    "\n",
    "    dfs(token)\n",
    "    return sorted(group, key=lambda x: x.i)\n",
    "\n",
    "def build_full_prep_phrase(prep_token, obj_token):\n",
    "    parts = [prep_token.text.lower()]\n",
    "    for child in obj_token.children:\n",
    "        if child.dep_ == \"case\":\n",
    "            parts.append(child.text.lower())\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def extract_spatial_relations(text, objects):\n",
    "    doc = nlp(text)\n",
    "    matcher = DependencyMatcher(nlp.vocab)\n",
    "    norms = normalize_objects(objects)\n",
    "    relations = set()\n",
    "\n",
    "    pattern_verb = [\n",
    "        {\"RIGHT_ID\": \"verb\", \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}},\n",
    "        {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"nsubj\", \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}},\n",
    "        {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"obl\", \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"obl\", \"nmod\"]}}},\n",
    "        {\"LEFT_ID\": \"obl\", \"REL_OP\": \">\", \"RIGHT_ID\": \"prep\", \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"case\", \"flat\", \"fixed\"]}}}\n",
    "    ]\n",
    "\n",
    "    pattern_nearby = [\n",
    "        {\"RIGHT_ID\": \"verb\", \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}},\n",
    "        {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"prep_adv\", \"RIGHT_ATTRS\": {\"DEP\": \"advmod\", \"POS\": \"ADV\"}},\n",
    "        {\"LEFT_ID\": \"prep_adv\", \"REL_OP\": \">\", \"RIGHT_ID\": \"obj\", \"RIGHT_ATTRS\": {\"DEP\": \"obl\"}},\n",
    "        {\"LEFT_ID\": \"obj\", \"REL_OP\": \">\", \"RIGHT_ID\": \"prep\", \"RIGHT_ATTRS\": {\"DEP\": \"case\"}},\n",
    "        {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"subj\", \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}},\n",
    "    ]\n",
    "\n",
    "    matcher.add(\"SPATIAL_VERB\", [pattern_verb])\n",
    "    matcher.add(\"SPATIAL_NEARBY\", [pattern_nearby])\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    def build_full_prep(tok):\n",
    "        parts = [tok.text.lower()]\n",
    "        parts += sorted([child.text.lower() for child in tok.children if child.dep_ in {\"fixed\", \"flat\", \"case\"}], key=lambda x: x)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    for match_id, tokens in matches:\n",
    "        match_label = nlp.vocab.strings[match_id]\n",
    "\n",
    "        if match_label == \"SPATIAL_VERB\":\n",
    "            verb, nsubj, obl, prep = [doc[i] for i in tokens]\n",
    "            prep_phrase = build_full_prep(prep)\n",
    "\n",
    "            subj_group = collect_conj_appos_group(nsubj)\n",
    "            obl_group = collect_conj_appos_group(obl)\n",
    "\n",
    "            subj_objs = [find_matching_object(tok, norms) for tok in subj_group]\n",
    "            obl_objs = [find_matching_object(tok, norms) for tok in obl_group]\n",
    "\n",
    "            subj_objs = [obj for obj in subj_objs if obj]\n",
    "            obl_objs = [obj for obj in obl_objs if obj]\n",
    "\n",
    "            for subj_obj in subj_objs:\n",
    "                for obl_obj in obl_objs:\n",
    "                    if subj_obj != obl_obj:\n",
    "                        relations.add((subj_obj, prep_phrase, obl_obj))\n",
    "\n",
    "        elif match_label == \"SPATIAL_NEARBY\":\n",
    "            verb, prep_adv, obj, prep, subj = [doc[i] for i in tokens]\n",
    "            #prep_phrase = build_full_prep(prep_adv)\n",
    "            prep_phrase = build_full_prep_phrase(prep_adv, obj)\n",
    "\n",
    "            subj_group = collect_conj_appos_group(subj)\n",
    "            subj_objs = [find_matching_object(tok, norms) for tok in subj_group]\n",
    "            subj_objs = [obj for obj in subj_objs if obj]\n",
    "\n",
    "            obl_obj = find_matching_object(obj, norms)\n",
    "\n",
    "            if obl_obj:\n",
    "                for subj_obj in subj_objs:\n",
    "                    if subj_obj != obl_obj:\n",
    "                        relations.add((subj_obj, prep_phrase, obl_obj))\n",
    "\n",
    "    return list(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9989887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c9205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8ddda149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основной путь к папке с батчами\n",
    "BATCH_DIR = \"dataset_text2json_spacy/texts\"\n",
    "LOG_FILE = \"error.log\"\n",
    "\n",
    "# Удаление дубликатов и сортировка по имени\n",
    "def get_ordered_batch_files():\n",
    "    return sorted(glob(os.path.join(BATCH_DIR, \"batch_*.txt\")))\n",
    "\n",
    "def get_existing_outputs():\n",
    "    return {os.path.basename(f).replace(\"dataset_\", \"batch_\").replace(\".jsonl\", \".txt\") \n",
    "            for f in glob(os.path.join(BATCH_DIR, \"dataset_*.jsonl\"))}\n",
    "\n",
    "def process_batch(batch_path, output_path):\n",
    "    with open(batch_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_file, open(LOG_FILE, \"a\", encoding=\"utf-8\") as log:\n",
    "        for idx, src_text in enumerate(tqdm(lines, desc=f\"Processing {os.path.basename(batch_path)}\", ncols=90)):\n",
    "            try:\n",
    "                norm_text = normalize_scene_text(src_text, PROMPT, API_KEY)\n",
    "                objects = extract_objects(norm_text)\n",
    "                attributes = extract_attributes(norm_text, objects)\n",
    "                relations = extract_spatial_relations(norm_text, objects)\n",
    "\n",
    "                item = {\n",
    "                    \"src_text\": src_text.strip(),\n",
    "                    \"norm_text\": norm_text,\n",
    "                    \"objects\": objects,\n",
    "                    \"attributes\": attributes,\n",
    "                    \"relations\": relations\n",
    "                }\n",
    "\n",
    "                out_file.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log.write(f\"[{os.path.basename(batch_path)}:{idx+1}] {type(e).__name__}: {str(e)}\\n\")\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6fbf85f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропущен уже обработанный файл: batch_1.txt\n",
      "Пропущен уже обработанный файл: batch_10.txt\n",
      "Пропущен уже обработанный файл: batch_11.txt\n",
      "Пропущен уже обработанный файл: batch_12.txt\n",
      "Пропущен уже обработанный файл: batch_13.txt\n",
      "Пропущен уже обработанный файл: batch_14.txt\n",
      "Пропущен уже обработанный файл: batch_15.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch_16.txt: 100%|██████████████████████████| 200/200 [04:58<00:00,  1.49s/it]\n",
      "Processing batch_17.txt: 100%|██████████████████████████| 200/200 [05:00<00:00,  1.50s/it]\n",
      "Processing batch_18.txt: 100%|██████████████████████████| 200/200 [04:46<00:00,  1.43s/it]\n",
      "Processing batch_19.txt: 100%|██████████████████████████| 200/200 [04:59<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропущен уже обработанный файл: batch_2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch_20.txt: 100%|██████████████████████████| 200/200 [05:00<00:00,  1.50s/it]\n",
      "Processing batch_21.txt: 100%|██████████████████████████| 200/200 [04:58<00:00,  1.49s/it]\n",
      "Processing batch_22.txt: 100%|██████████████████████████| 200/200 [05:16<00:00,  1.58s/it]\n",
      "Processing batch_23.txt: 100%|██████████████████████████| 200/200 [04:55<00:00,  1.48s/it]\n",
      "Processing batch_24.txt: 100%|██████████████████████████| 200/200 [04:59<00:00,  1.50s/it]\n",
      "Processing batch_25.txt: 100%|██████████████████████████| 203/203 [05:03<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропущен уже обработанный файл: batch_3.txt\n",
      "Пропущен уже обработанный файл: batch_4.txt\n",
      "Пропущен уже обработанный файл: batch_5.txt\n",
      "Пропущен уже обработанный файл: batch_6.txt\n",
      "Пропущен уже обработанный файл: batch_7.txt\n",
      "Пропущен уже обработанный файл: batch_8.txt\n",
      "Пропущен уже обработанный файл: batch_9.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_files = get_ordered_batch_files()\n",
    "processed_files = get_existing_outputs()\n",
    "\n",
    "for batch_path in batch_files:\n",
    "    filename = os.path.basename(batch_path)\n",
    "    if filename in processed_files:\n",
    "        print(f\"Пропущен уже обработанный файл: {filename}\")\n",
    "        continue\n",
    "\n",
    "    batch_num = filename.replace(\"batch_\", \"\").replace(\".txt\", \"\")\n",
    "    output_path = os.path.join(BATCH_DIR, f\"dataset_{batch_num}.jsonl\")\n",
    "    process_batch(batch_path, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518ddc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
